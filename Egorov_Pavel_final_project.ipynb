{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geopard26/MDS/blob/main/Egorov_Pavel_final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CnUdDZ4onHd"
      },
      "source": [
        "# Probability\n",
        "## Final project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVsd_-pwonHh"
      },
      "source": [
        "In the final project you will develop your own classification algorithm based on probabilistic models.\n",
        "\n",
        "In `data.zip`you can find a collection of texts in English, Italian, Spanish, German, French, Polish and Portuguese languages obtained from random Wikipedia articles, see e.g. `Spanish.txt`. (See corresponding `.source.txt` files and links therein for lists of authors.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGomL4UnonHj"
      },
      "outputs": [],
      "source": [
        "with open(\"Spanish.txt\") as f:\n",
        "    print(f.read(500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in7YYpE4onHk"
      },
      "outputs": [],
      "source": [
        "with open(\"German.txt\") as f:\n",
        "    print(f.read(500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSlexPmBonHl"
      },
      "source": [
        "This is our training data. These texts are preprocessed: only standard Latin characters kept, diacritics removed, punctuations removed, all letters converted to lowercase. We will use similar preprocessing for new texts that we have to classify. Here are some useful functions to do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YtkQsA0onHm"
      },
      "source": [
        "### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "9pmmgKsZonHn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "### FROM: https://stackoverflow.com/a/518232/3025981\n",
        "def strip_accents(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "### END FROM\n",
        "\n",
        "def clean_text(s):\n",
        "    return re.sub(\"[^a-z \\n]\", \"\", strip_accents(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or23-0YeonHo"
      },
      "source": [
        "### Learning: obtaining character frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PncO_AQonHp"
      },
      "source": [
        "First of all, we have to find character relative frequencies in texts of each language. We will consider them as probability for character to appear in our multinomial model. Write function `get_freqs(text, relative)` that takes string `text` as input and returns dictionary which keys are all distinct characters occurred in `text` and values are frequencies (relative if `relative` is `True` and absolute otherwise). A similar function was discussed in Python videos previously. Note that Python treat `str` objects as a sequence of characters (e.g. if you try to iterate it)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-bcf26fb5354210c1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "JBA_XsgYonHq"
      },
      "outputs": [],
      "source": [
        "def get_freqs(text, relative=False):\n",
        "    ### BEGIN SOLUTION\n",
        "    freqs = {}\n",
        "\n",
        "    # Iterate over each character in the text\n",
        "    for char in text:\n",
        "        if char in freqs:\n",
        "            freqs[char] += 1\n",
        "        else:\n",
        "            freqs[char] = 1\n",
        "\n",
        "    if relative:\n",
        "        total_chars = len(text)\n",
        "        freqs = {char: count / total_chars for char, count in freqs.items()}\n",
        "\n",
        "    return freqs\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-122fc498050c5088",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "swbANAoSonHq"
      },
      "outputs": [],
      "source": [
        "assert get_freqs('Hello, World!') == {'H': 1, 'e': 1, 'l': 3, 'o': 2, ',': 1,\n",
        "                                      ' ': 1, 'W': 1, 'r': 1, 'd': 1, '!': 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60jvwbNuonHr"
      },
      "source": [
        "Now use function `get_freqs` to create a dictionary `lang_to_prob` which keys are names of languages (i.e. `'English'`, `'Italian'`, `'Spanish'`, `'German'`, `'French'`, `'Polish'`, `'Portuguese'`) and values are dictionaries of relative frequencies, obtained by processing of corresponding `.txt` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1beba6e8ac03b0b0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "z9DuhKWQonHr"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "def create_lang_to_prob(lang_files):\n",
        "    lang_to_prob = {}\n",
        "    for lang, file_path in lang_files.items():\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            freqs = get_freqs(text, relative=True)\n",
        "            lang_to_prob[lang] = freqs\n",
        "    return lang_to_prob\n",
        "\n",
        "lang_files = {\n",
        "    'English': 'English.txt',\n",
        "    'Italian': 'Italian.txt',\n",
        "    'French': 'French.txt',\n",
        "    'German': 'German.txt',\n",
        "    'Polish': 'Polish.txt',\n",
        "    'Portuguese': 'Portuguese.txt',\n",
        "    'Spanish': 'Spanish.txt'\n",
        "}\n",
        "\n",
        "lang_to_probs = create_lang_to_prob(lang_files)\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-56feb4b2ab62c15c",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "K_6dUv1JonHs"
      },
      "outputs": [],
      "source": [
        "assert abs(lang_to_probs['Polish']['a'] - 0.08504245058355897) < 0.00001\n",
        "assert abs(lang_to_probs['English']['x'] - 0.001387857977519179) < 1e-5\n",
        "assert len(set(lang_to_probs['Portuguese'])) == 28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IT9Az6EonHs"
      },
      "source": [
        "### Likelihoods\n",
        "Now let us start implementing the actual classifier. We begin with multinomial likelihood function. Implement function `multinomial_likelihood(probs, freqs)` that takes two arguments: `probs` are dictionary of probabilities of each character (in some language) and `freqs` is dictionary of absolute frequencies of each character (in some text we want to classify). This function have to return probability to obtain these absolute frequencies from multinomial distribution with given probabilities $P((X_1 = f_1) \\cap (X_2 = f_2) \\cap \\ldots \\cap (X_k = f_k))$ provided that $(X_1, \\ldots, X_k)$ is a system of multinomially distributed values with probabilities $(p_1, \\ldots, p_k)$. (You need `factorial` function that can be imported from `math`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9a0431bd007d04cf",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "KS4lto2KonHs"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "from math import factorial\n",
        "\n",
        "def multinomial_likelihood(probs, freqs):\n",
        "    n = sum(freqs.values())  # Total number of observations\n",
        "    likelihood_coefficient = factorial(n)\n",
        "\n",
        "    for char, freq in freqs.items():\n",
        "        likelihood_coefficient /= factorial(freq)\n",
        "        likelihood_coefficient *= probs.get(char, 0) ** freq\n",
        "\n",
        "    return likelihood_coefficient\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q3K8je2onHt"
      },
      "source": [
        "Let's find likelihood of data with frequencies `{'a': 2, 'b': 1, 'c': 2}` and probabilities `{'a': 0.2, 'b': 0.5, 'c': 0.3}`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "QoqTTC0ionHt",
        "outputId": "6fee89ec-2f3c-4388-abc1-556e313ac0c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.054000000000000006"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ],
      "source": [
        "multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3}, freqs={'a': 2, 'b': 1, 'c': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-0a8a6d99346b6bf8",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2VYIB_LronHt"
      },
      "outputs": [],
      "source": [
        "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.054) < 0.000001\n",
        "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.1, 'c': 0.3, 'd': 0.4},\n",
        "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.0108) < 0.000001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csAFr_YonHu"
      },
      "source": [
        "Note that the coefficient with factorials depends only on `freqs` (i.e. text that we analyse) and does not depend on `probs`. It means that for all possible languages this coefficient will be the same. As we are going to consider fixed text and compare likelihoods for different languages, we see, that we do not need this coefficient in most of cases. Let us implement function `multinomial_likelihood_without_coeff` that returns the same probability as `multinomial_likelihood` but without coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-99bfc1e01b6b804e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "CzH6sstIonHu"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "from numpy import power\n",
        "\n",
        "def multinomial_likelihood_without_coeff(probs, freqs):\n",
        "    p = 1\n",
        "    for c in freqs:\n",
        "        p *= power(probs[c], freqs[c])\n",
        "    return p\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH5vtbPhonHu"
      },
      "source": [
        "Corresponding probability become smaller:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "AWxoGtbKonHu",
        "outputId": "af1ad42b-5f15-4f2f-de37-3c5c23fa0dc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0018000000000000004"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ],
      "source": [
        "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                                     freqs={'a': 2, 'b': 1, 'c': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-3de0433027c5af68",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "RVn4twczonHv"
      },
      "outputs": [],
      "source": [
        "assert multinomial_likelihood_without_coeff(\n",
        "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
        "    freqs={'a': 2, 'b': 1, 'c': 2}) == 0.00324\n",
        "assert abs(multinomial_likelihood_without_coeff(\n",
        "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
        "    freqs={'a': 2, 'b': 1, 'c': 5}) - 8.747999999999e-05) < 1e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j68-jIAonHv"
      },
      "source": [
        "Actually, likelihoods become extremely small very quickly when we increase absolute frequencies in data. It is not surprisingly: the probability to get the text that coincides with our actual text from random experiment we discussed is extremely small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "QC4HF-UaonHv",
        "outputId": "6015bcbd-2b4b-4dc3-cbd6-a30062625f73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00018000000000000004"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ],
      "source": [
        "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                    freqs={'a': 3, 'b': 2, 'c': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "8nSWUd2fonHv",
        "outputId": "82e41aea-1b3b-4de2-f94d-e77cef74ec4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.866455078125001e-10"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ],
      "source": [
        "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                    freqs={'a': 3, 'b': 20, 'c': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i57kmLponHv"
      },
      "source": [
        "Due to limited precision of computer arithmetic, for frequencies large enough we will get exactly zero likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "cWeiFPtvonHv",
        "outputId": "1181d6ea-952f-4efb-882d-73a72b944dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ],
      "source": [
        "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                                     freqs={'a': 543, 'b': 512, 'c': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzglPanqonHw"
      },
      "source": [
        "Thus we usually cannot use likelihoods like this directly. Common way to deal with such a tiny numbers is to use _logarithms_ instead of likelihood themself. Indeed, logarithm is monotonically increasing function. If we compare logarithms, it is equivalent to compare their arguments.\n",
        "\n",
        "### Log likelihood\n",
        "Implement function `log_likelihood_without_coeff(probs, freqs)` that calculates logarithm of likelihood (without factorial coefficient). Note that you cannot simply put `multinomial_likelihood_without_coeff` into `log`: if the likelihood would be extremely small (equal to zero from computer's point of view), `log` of it will be negative infinity. Thus we have to algebraically transform the expression for log likelihood first. Let us use properties of logarithm to do it: $\\log (ab) = \\log a + \\log b$, $\\log (a^b)=b\\log a$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-c6283b47bbcf5b95",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "DjeFHu8MonHw"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "from math import log\n",
        "\n",
        "def log_likelihood_without_coeff(probs, freqs):\n",
        "    p = 0\n",
        "    for c in freqs:\n",
        "        p += freqs[c] * log(probs[c])\n",
        "    return p\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQyt3Yu9onHw"
      },
      "source": [
        "Likelihoods are probabilities, so they are less than 1 and their logarithms are negative. The larger absolute value of log-likelihood, the less is likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "5bS5HxvponHw",
        "outputId": "ca40a3cb-5108-40d0-9cdf-fc3903546ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-6.319968614080018"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ],
      "source": [
        "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                             freqs={'a': 2, 'b': 1, 'c': 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E5IspSonHw"
      },
      "source": [
        "Now we can deal with inputs that lead to exact zero value previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "iToxFlQxonHw",
        "outputId": "aeb6fa47-85ba-4dc3-d46c-a087433284a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1231.2240885070605"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ],
      "source": [
        "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                             freqs={'a': 543, 'b': 512, 'c': 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-38d4478fc7c1656b",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "6yjncsWSonHx"
      },
      "outputs": [],
      "source": [
        "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                              freqs={'a': 2, 'b': 1, 'c': 2}) + 6.319968614080018) < 0.00001\n",
        "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                             freqs={'a': 543, 'b': 512, 'c': 2}) + 1231.2240885070605) < 0.0001\n",
        "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
        "                             freqs={'a': 543, 'b': 512}) + 1228.8161428984085) < 0.0001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36qOaUfbonHx"
      },
      "source": [
        "### Theoretical question: cross-entropy and Jensen's inequality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq0kYJJUonHx"
      },
      "source": [
        "The function that you obtained is also know as *cross entropy* and is extremely popular in machine learning, namely, in classification problems. It allows you to measure how good probability distribution $(p_1, \\ldots, p_k)$ fits to actual absolute frequencies obtained from the data $(f_1, \\ldots, f_k)$.\n",
        "\n",
        "Assume that frequencies $(f_1, \\ldots, f_k)$ are fixed. What is the best distribution $(p_1, \\ldots, p_k)$ from likelihood's perspective?\n",
        "\n",
        "Intuitively, it seems that we have to put relative frequencies\n",
        "$$r_i = \\frac{f_i}{\\sum_{j=1}^k f_j}$$\n",
        "as $p_i$ to get best fit. In fact, it is true. To prove it, let us use Jensen's inequality for logarithms. It is stated as follows:\n",
        "\n",
        "For any values $\\alpha_1, \\ldots, \\alpha_k$, such that $\\sum_{j=1}^k \\alpha_j = 1$ and $\\alpha_j \\ge 0$ for all $j=1, \\ldots, k$, and any positive values $x_1, \\ldots, x_k$, the following inequality holds:\n",
        "\n",
        "$$\\log \\sum_{j=1}^k \\alpha_j x_j \\ge \\sum_{j=1}^k \\alpha_j\\log(x_j).$$\n",
        "\n",
        "Use this inequality to prove that\n",
        "$$\\sum_{j=1}^k r_j \\log p_j - \\sum_{j=1}^k r_j \\log r_j \\le 0,$$\n",
        "then prove that to obtain maximum log-likelihood (and therefore maximum likelihood) for fixed $(f_1, \\ldots, f_k)$ we have to put $p_i=r_i$, $i=1,\\ldots, k$.\n",
        "\n",
        "**Hint:** use properties of logarithm to transform left-hand part of the last inequality to right-rand part of the previous inequality.\n",
        "\n",
        "(Submit your proof as staff graded assignment.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution\n",
        "\n",
        "### Proof of Using Relative Frequencies to Maximize Log-Likelihood\n",
        "\n",
        "To prove that the best probability distribution $(p_1, \\ldots, p_k)$ corresponds to the relative frequencies $(r_1, \\ldots, r_k)$, where $r_i = \\frac{f_i}{\\sum_{j=1}^k f_j}$, we will use Jensen's Inequality for concave functions, such as the logarithmic function.\n",
        "\n",
        "**Application of Jensen's Inequality**:\n",
        "\n",
        "Jensen's Inequality states that for any numbers $\\alpha_1, \\ldots, \\alpha_k$, where $\\sum_{j=1}^k \\alpha_j = 1$ and $\\alpha_j \\ge 0$, and any positive numbers $x_1, \\ldots, x_k$, the following inequality holds:\n",
        "\n",
        "$$\\log \\sum_{j=1}^k \\alpha_j x_j \\ge \\sum_{j=1}^k \\alpha_j\\log(x_j).$$\n",
        "\n",
        "In our case, let $\\alpha_j = r_j$ and $x_j = \\frac{p_j}{r_j}$. Applying Jensen's Inequality, we get:\n",
        "\n",
        "$$ \\log \\sum_{j=1}^k r_j \\cdot \\frac{p_j}{r_j} \\ge \\sum_{j=1}^k r_j \\log \\frac{p_j}{r_j} $$\n",
        "\n",
        "Since $\\sum_{j=1}^k r_j = 1$ and $\\sum_{j=1}^k p_j = 1$, the left side of the inequality simplifies to $\\log(1) = 0$.\n",
        "\n",
        "**Choosing $p_j = r_j$ to Maximize Log-Likelihood**:\n",
        "\n",
        "Thus, $\\sum_{j=1}^k r_j \\log \\frac{p_j}{r_j} \\le 0$. Equality is achieved if and only if $\\frac{p_j}{r_j} = 1$ for all $j$, i.e., when $p_j = r_j$ for all $j$. This condition corresponds to maximum log-likelihood because otherwise, the log-likelihood would be less than zero.\n",
        "\n",
        "Therefore to maximize the log-likelihood for fixed frequencies $(f_1, \\ldots, f_k)$, it is necessary to choose $p_j = r_j$ for all $j$.\n"
      ],
      "metadata": {
        "id": "aQM0ASW-nzJY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSyyRqBnonHx"
      },
      "source": [
        "### Maximum likelihood classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl0iOhJPonHx"
      },
      "source": [
        "Now we will use likelihood to choose the best language for some text we want to classify. Write function `mle_best(text, lang_to_probs)` that takes some `text` and dictionary `lang_to_probs` that we created previously and returns the name of language such that likelihood of our data for this language is maximal. Note that you have to preprocess  `text` using function `clean_text` before finding frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-761e62cc0f17dc2d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dM8wxxcRonHx"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "def mle_best(text, lang_to_probs):\n",
        "    # Preprocess the text\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    # Calculate frequencies of characters in the cleaned text\n",
        "    freqs = get_freqs(cleaned_text)\n",
        "\n",
        "    # Initialize variables to store the best language and its log likelihood\n",
        "    best_lang = None\n",
        "    # best_log_likelihood = float('-inf')\n",
        "    highest_p = 0\n",
        "\n",
        "    for lang in lang_to_probs:\n",
        "        log_likelihood = multinomial_likelihood_without_coeff(lang_to_probs[lang], freqs)\n",
        "        if log_likelihood > highest_p:\n",
        "            best_lang = lang\n",
        "            highest_p = log_likelihood\n",
        "    return best_lang\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5BY3K9onHx"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "zcIgYjYEonHx",
        "outputId": "f7c0988c-8fca-46b1-c062-4152c591d984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'English'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 254
        }
      ],
      "source": [
        "# Source: https://en.wikipedia.org/wiki/1134_Kepler\n",
        "text_en = \"\"\"1134 Kepler, provisional designation 1929 SA, is a stony asteroid\n",
        "and eccentric Mars-crosser from the asteroid belt, approximately\n",
        "4 kilometers in diameter\"\"\"\n",
        "mle_best(text_en, lang_to_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "eebo3vi7onHy",
        "outputId": "00078f14-1407-4235-fcad-65326e1558d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Polish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 255
        }
      ],
      "source": [
        "# Source: https://pl.wikipedia.org/wiki/(1134)_Kepler\n",
        "text = \"\"\"\"(1134) Kepler – planetoida z grupy przecinających\n",
        "orbitę Marsa okrążająca Słońce w ciągu 4 lat i 145 dni\n",
        "w średniej odległości 2,68 au.\n",
        "\"\"\"\n",
        "mle_best(text, lang_to_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "z5-vM4T5onHy",
        "outputId": "45649350-ab3d-42f2-b83b-6b229bf9203b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Italian'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 256
        }
      ],
      "source": [
        "# Source: https://it.wikipedia.org/wiki/1134_Kepler\n",
        "text = \"\"\"1134 Kepler è un asteroide areosecante. Scoperto nel 1929,\n",
        "presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098\n",
        "UA e da un'eccentricità di 0,4651458, inclinata di 15,17381° rispetto\n",
        "\"\"\"\n",
        "mle_best(text, lang_to_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-b83cf8d4bcee2ab1",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2_0M8_p8onHy"
      },
      "outputs": [],
      "source": [
        "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
        "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
        "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
        "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
        "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
        "         \"and eccentric Mars-crosser from the\"]\n",
        "assert mle_best(lines[0], lang_to_probs) == 'Portuguese'\n",
        "assert mle_best(lines[1], lang_to_probs) == 'Portuguese'\n",
        "assert mle_best(lines[2], lang_to_probs) == 'French'\n",
        "assert mle_best(lines[3], lang_to_probs) == 'Italian'\n",
        "assert mle_best(lines[4], lang_to_probs) == 'Polish'\n",
        "assert mle_best(lines[5], lang_to_probs) == 'English'\n",
        "assert mle_best(\"Aaaa\", lang_to_probs) == \"Portuguese\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7dyK3uJonHy"
      },
      "source": [
        "### Look, it works!\n",
        "Fantastic! Just a couple of equations, and we created fully automatic language detection algorithm!\n",
        "\n",
        "Now let us make it even better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYWQTULZonHy"
      },
      "source": [
        "### Let's go Bayes\n",
        "Assume that we selected random article from all Wikipedia articles in languages that we consider. There are different number of articles in different languages, so it is more likely to obtain article e.g. in English than in Polish (at least, at this moment). It means that we need stronger evidence for Polish to accept it comparing with English. To take into account this information, we will use Bayes' rule as discussed in the videos.\n",
        "\n",
        "Recall that in Bayessian approach we consider _prior_ probabilities of languages, then use Bayes' rule to find their posterior probabilies and select language with largest posterior probability. We begin with finding prior probabilities. Create a dictionary `lang_to_prior` which keys are language names and values are prior probabilities. Assume that priors are prortional to number of articles in each language (in thousands). Let us use these numbers (in thousands): English: 6090, Italian: 1611, Spanish: 1602, German: 2439, French: 2222, Polish: 1412, Portuguese: 1034. Remember that all prior probabilities should sum up to 1!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1076e90562dad99c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "RnDs7oOeonHy"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "# Numbers of articles for each language (in thousands)\n",
        "articles = {\n",
        "    'English': 6090,\n",
        "    'Italian': 1611,\n",
        "    'Spanish': 1602,\n",
        "    'German': 2439,\n",
        "    'French': 2222,\n",
        "    'Polish': 1412,\n",
        "    'Portuguese': 1034\n",
        "}\n",
        "\n",
        "# Calculate the total number of articles\n",
        "total_articles = sum(articles.values())\n",
        "\n",
        "# Calculate prior probabilities\n",
        "lang_to_prior = {lang: count / total_articles for lang, count in articles.items()}\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-19765ecb964a7f7f",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "o4QVmRdzonHz"
      },
      "outputs": [],
      "source": [
        "assert abs(lang_to_prior['French'] - 0.13540524070688603) < 0.000001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXhgYKFJonHz"
      },
      "source": [
        "Now implement function `bayesian_best(text, lang_to_probs, lang_to_prior)` that takes some text `text`, dictionary `lang_to_probs` created before and `lang_to_prior` with prior probabilities. This function have to return language name with largest posterior probability. Note that as we only compare posterior probabilities, we can ignore the denominator in Bayes' rule: it is the same for all languages. Note also that we need to work with logarithms of posterior instead of posteriors themself to avoid extremely small numbers. Use properties of logarithm to do it effeciently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1725db02a8cb77ad",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "A7AFgrY9onHz"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "import math\n",
        "\n",
        "def bayesian_best(text, lang_to_probs, lang_to_prior):\n",
        "    text = clean_text(text)\n",
        "    freqs = get_freqs(text)\n",
        "    best_lang = None\n",
        "    highest_p = 0\n",
        "    for lang in lang_to_probs:\n",
        "        probability_test = multinomial_likelihood_without_coeff(lang_to_probs[lang], freqs)\n",
        "        probability_test *= lang_to_prior[lang]\n",
        "        if probability_test > highest_p:\n",
        "            best_lang = lang\n",
        "            highest_p = probability_test\n",
        "    return best_lang\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro9QObUJonHz"
      },
      "source": [
        "For example, MLE algorithm believes that word `\"The\"` belongs go German language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N6_gz6rbonHz",
        "outputId": "ac2219fd-9d85-4ee7-9053-8b2441dc8e89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'German'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 262
        }
      ],
      "source": [
        "mle_best(\"The\", lang_to_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCH74x-GonHz"
      },
      "source": [
        "However, if we take into account that English is more popular in Wikipdia, results changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "v8LfC6ToonHz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e4068662-4f34-442b-9a26-5e1b47433910"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'English'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 263
        }
      ],
      "source": [
        "bayesian_best(\"The\", lang_to_probs, lang_to_prior)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bayesian_best(\"Kepler è un asteroide areosecante. Scoperto nel\", lang_to_probs, lang_to_prior)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "swH2n4UhRFSH",
        "outputId": "6e371b7f-81bc-4727-e148-8c4b4423022c"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'French'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-d6b6b54026f24fb0",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "KvlT6sBConHz"
      },
      "outputs": [],
      "source": [
        "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
        "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
        "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
        "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
        "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
        "         \"and eccentric Mars-crosser from the\"]\n",
        "answers = ['English', 'English', 'French', 'Italian', 'Polish', 'English']\n",
        "for line, answer in zip(lines, answers):\n",
        "    assert bayesian_best(line, lang_to_probs, lang_to_prior) == answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9RMCkxfonH0"
      },
      "source": [
        "### Measuring uncertainty\n",
        "Finally, let us get posterior probability how certain our Bayesian algorithm in its classification. Implement function `bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang)` that takes some `text`, `lang_to_probs`, `lang_to_prior` and a particular language `test_lang` we are interested in and returns posterior probability for this language provided this text. Use formula for the posterior from the videos.\n",
        "\n",
        "Note that in this case you have to work with actual probabilites (likelihoods), not their logarithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-8705f891f8eff4f7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "c0epuiy4onH0"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTIONS\n",
        "def bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang):\n",
        "  # Preprocess the text and calculate relative frequencies\n",
        "    text = clean_text(text)\n",
        "    freqs = get_freqs(text)\n",
        "   # Calculate the posterior probability for the test language\n",
        "    test_lang_posterior = multinomial_likelihood_without_coeff(lang_to_probs[test_lang], freqs)\n",
        "    test_lang_posterior *= lang_to_prior[test_lang]\n",
        "    total_posterior = 0\n",
        "  # Calculate the sum of posterior probabilities for all languages\n",
        "    for lang in lang_to_probs:\n",
        "        lang_likelihood = multinomial_likelihood_without_coeff(lang_to_probs[lang], freqs)\n",
        "        lang_likelihood *= lang_to_prior[lang]\n",
        "        total_posterior += lang_likelihood\n",
        "    return test_lang_posterior / total_posterior\n",
        "### END SOLUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "mAOwAYnponH0",
        "outputId": "7e4e6840-d846-41ba-bcd5-dca0e5c81c77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2686381464045339"
            ]
          },
          "metadata": {},
          "execution_count": 267
        }
      ],
      "source": [
        "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "sUVZPTiPonH0",
        "outputId": "fbc73159-d985-4ea3-d6d3-0eca44409233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.534626604689872"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ],
      "source": [
        "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "NdIeqtygonH0",
        "outputId": "ec60ddfa-a326-4a8a-8196-d5392a4341c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36596793151737517"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ],
      "source": [
        "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "pm21A6zvonH0",
        "outputId": "41fb7276-766e-451c-8752-1f11c31c82a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12095519926831602"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ],
      "source": [
        "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"German\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-23ecc58928d8d618",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "fsuKSxNmonH0"
      },
      "outputs": [],
      "source": [
        "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\") - 0.26863814640) < 0.00001\n",
        "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\") - 0.534626604) < 0.00001\n",
        "assert abs(bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\") - 0.365967931517) < 0.00001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NY-Bm5lonH0"
      },
      "source": [
        "We see that our algorithm believes that `\"Das\"` belongs to English. This is due to small amount of data (only three letters!) and high prior for English. However, it is not very certain: the posterior is only 0.37. On the other hand, `\"The\"` belongs to `\"English\"` with much larger posterior probability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kiewa34NonH0"
      },
      "source": [
        "## Conclusions\n",
        "Today we used our knowledge of probability to construct our own classifier algorithm. Actually, it is a version of well-known naive Bayesian classifier. Of course, this classifier is far from perfect: for example, it completely ignores words and deal only with characters and their frequencies. Nevertheless, it works rather good despite being so simple. Also it shows several important concepts: likelihood, maximum likelihood estimation, Bayesian estimations and so on.\n",
        "\n",
        "Now you are ready for more sophisticated topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovy3GARjonH0"
      },
      "source": [
        "### P.S. Efficiency considerations\n",
        "During this project we used dictionaries to represent frequency tables and pure Python constructions like loops to process them. However, it is much more effecient to use `numpy` arrays and vectorized functions. You can try to redo this project with `numpy` if you know how to use it. If you don't know yet, that's not a problem: you will learn it soon."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}